import platform
import warnings

import matplotlib

matplotlib.use("Agg")

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import sklearn
from IPython.display import display
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import (
    adjusted_rand_score,
    calinski_harabasz_score,
    davies_bouldin_score,
    silhouette_samples,
    silhouette_score,
)
from sklearn.preprocessing import LabelEncoder, StandardScaler

warnings.filterwarnings("ignore")

# –≠–¢–ê–ü 1: –ü–û–°–¢–ê–ù–û–í–ö–ê –ó–ê–î–ê–ß–ò –ò –î–ê–ù–ù–´–ï

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
PALETTE = "Set2"
sns.set(style="whitegrid")
plt.rcParams["figure.figsize"] = (12, 6)
plt.rcParams["font.size"] = 12

RANDOM_STATE = 42


print("\n--- –≠–¢–ê–ü 1: –ü–û–°–¢–ê–ù–û–í–ö–ê –ó–ê–î–ê–ß–ò –ò –î–ê–ù–ù–´–ï ---\n")

try:
    url = "https://drive.google.com/uc?id=1dvVgFSH22J7okTKYD8sHzJvJJ9MRZzkN&export=download"
    df = pd.read_csv(url, delimiter=";")
    print("‚úì –§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
except Exception as e:
    print(f"‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}")
    exit(1)

COLUMN_TRANSLATOR = {
    "age": "–í–æ–∑—Ä–∞—Å—Ç",
    "job": "–ü—Ä–æ—Ñ–µ—Å—Å–∏—è",
    "marital": "–°–µ–º–µ–π–Ω–æ–µ –ø–æ–ª–æ–∂–µ–Ω–∏–µ",
    "education": "–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ",
    "default": "–ö—Ä–µ–¥–∏—Ç–Ω—ã–π –¥–µ—Ñ–æ–ª—Ç",
    "housing": "–ò–ø–æ—Ç–µ–∫–∞",
    "loan": "–ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏–π –∫—Ä–µ–¥–∏—Ç",
    "contact": "–¢–∏–ø –∫–æ–Ω—Ç–∞–∫—Ç–∞",
    "month": "–ú–µ—Å—è—Ü",
    "day_of_week": "–î–µ–Ω—å –Ω–µ–¥–µ–ª–∏",
    "duration": "–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–≤–æ–Ω–∫–∞ (—Å–µ–∫)",
    "campaign": "–ö–æ–ª-–≤–æ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –≤ –∫–∞–º–ø–∞–Ω–∏–∏",
    "pdays": "–î–Ω–µ–π —Å –ø—Ä–æ—à–ª–æ–≥–æ –∫–æ–Ω—Ç–∞–∫—Ç–∞",
    "previous": "–ö–æ–ª-–≤–æ –ø—Ä–æ—à–ª—ã—Ö –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤",
    "poutcome": "–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ—à–ª–æ–π –∫–∞–º–ø–∞–Ω–∏–∏",
    "emp.var.rate": "–ò–∑–º. —É—Ä–æ–≤–Ω—è –∑–∞–Ω—è—Ç–æ—Å—Ç–∏",
    "cons.price.idx": "–ò–Ω–¥–µ–∫—Å –ø–æ—Ç—Ä–µ–±. —Ü–µ–Ω",
    "cons.conf.idx": "–ò–Ω–¥–µ–∫—Å –ø–æ—Ç—Ä–µ–±. –¥–æ–≤–µ—Ä–∏—è",
    "euribor3m": "–°—Ç–∞–≤–∫–∞ Euribor 3M",
    "nr.employed": "–ß–∏—Å–ª–æ –∑–∞–Ω—è—Ç—ã—Ö",
    "y": "–°–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –≤–∫–ª–∞–¥",
}

print("\n–û–ü–ò–°–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê:")
print("-" * 50)
print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {df.shape[0]} —Å—Ç—Ä–æ–∫ √ó {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")

print("\n–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
display(df.head())

print("\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–∏–ø–∞—Ö –¥–∞–Ω–Ω—ã—Ö:")
df.info()

# –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
df.drop_duplicates(inplace=True)
print(f"‚úì –î—É–±–ª–∏–∫–∞—Ç—ã —É–¥–∞–ª–µ–Ω—ã. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {df.shape}")

# –û–±—Ä–∞–±–æ—Ç–∫–∞ 'unknown' –∑–Ω–∞—á–µ–Ω–∏–π
object_cols = df.select_dtypes(include=["object"]).columns
for col in object_cols:
    if "unknown" in df[col].unique():
        mode_value = df[col].mode()[0]
        if mode_value == "unknown":
            second_mode = df[col].value_counts().index[1]
            df[col] = df[col].replace("unknown", second_mode)
        else:
            df[col] = df[col].replace("unknown", mode_value)
print("‚úì –ó–Ω–∞—á–µ–Ω–∏—è 'unknown' –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ –º–æ–¥—É")

# –î–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–ª–∏–µ–Ω—Ç–∞
clustering_features = [
    "age",
    "job",
    "marital",
    "education",
    "default",
    "housing",
    "loan",
    "campaign",
    "pdays",
    "previous",
    "poutcome",
]

df_cluster = df[clustering_features].copy()
print(f"\n‚úì –í—ã–±—Ä–∞–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {len(clustering_features)} —à—Ç.")
print(f"  {clustering_features}")


# –≠–¢–ê–ü 2: –ë–ê–ó–û–í–´–ô –ú–ï–¢–û–î (BASELINE) - K-MEANS

print("–≠–¢–ê–ü 2: –ë–ê–ó–û–í–´–ô –ú–ï–¢–û–î (BASELINE) - K-MEANS –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø")


df_encoded = df_cluster.copy()

# Label Encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
label_encoders = {}
categorical_cols = df_encoded.select_dtypes(include=["object"]).columns

for col in categorical_cols:
    le = LabelEncoder()
    df_encoded[col] = le.fit_transform(df_encoded[col])
    label_encoders[col] = le
    print(f"‚úì {col}: –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ {len(le.classes_)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π")

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_encoded)
print("\n‚úì –î–∞–Ω–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω—ã (StandardScaler)")

# –ú–µ—Ç–æ–¥ –ª–æ–∫—Ç—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
print("\n--- –ú–µ—Ç–æ–¥ –ª–æ–∫—Ç—è (Elbow Method) ---")

inertias = []
silhouettes = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)
    silhouettes.append(silhouette_score(X_scaled, kmeans.labels_))
    print(f"k={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={silhouettes[-1]:.4f}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–∞ –ª–æ–∫—Ç—è
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].plot(K_range, inertias, "bo-", linewidth=2, markersize=8)
axes[0].set_xlabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (k)")
axes[0].set_ylabel("–ò–Ω–µ—Ä—Ü–∏—è (Inertia)")
axes[0].set_title("–ú–µ—Ç–æ–¥ –ª–æ–∫—Ç—è: –ò–Ω–µ—Ä—Ü–∏—è vs –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
axes[0].grid(True, alpha=0.3)

axes[1].plot(K_range, silhouettes, "go-", linewidth=2, markersize=8)
axes[1].set_xlabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (k)")
axes[1].set_ylabel("Silhouette Score")
axes[1].set_title("Silhouette Score vs –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig("elbow_method.png", dpi=150, bbox_inches="tight")
plt.show()

print("\nüìä [–°–ö–†–ò–ù–®–û–¢ 1: –ú–µ—Ç–æ–¥ –ª–æ–∫—Ç—è - elbow_method.png]")

optimal_k = 4
print(f"\n‚úì –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: k = {optimal_k}")

# –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ KMeans (BASELINE)
print("\n--- –û–±—É—á–µ–Ω–∏–µ KMeans (BASELINE) —Å k={} ---".format(optimal_k))

kmeans_final = KMeans(n_clusters=optimal_k, random_state=RANDOM_STATE, n_init=10)
kmeans_labels = kmeans_final.fit_predict(X_scaled)

df["cluster_kmeans"] = kmeans_labels

# –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è baseline
baseline_silhouette = silhouette_score(X_scaled, kmeans_labels)
baseline_inertia = kmeans_final.inertia_
baseline_davies_bouldin = davies_bouldin_score(X_scaled, kmeans_labels)
baseline_calinski = calinski_harabasz_score(X_scaled, kmeans_labels)

print(f"‚úì KMeans (BASELINE) –æ–±—É—á–µ–Ω")
print(f"\n–ú–µ—Ç—Ä–∏–∫–∏ BASELINE (KMeans):")
print(f"  ‚Ä¢ Silhouette Score: {baseline_silhouette:.4f}")
print(f"  ‚Ä¢ Inertia: {baseline_inertia:.2f}")
print(f"  ‚Ä¢ Davies-Bouldin Index: {baseline_davies_bouldin:.4f}")
print(f"  ‚Ä¢ Calinski-Harabasz Index: {baseline_calinski:.2f}")

print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:")
print(pd.Series(kmeans_labels).value_counts().sort_index())


# –≠–¢–ê–ü 3: –ü–†–û–î–í–ò–ù–£–¢–´–ï –ú–ï–¢–û–î–´ (DBSCAN –∏ Hierarchical Clustering)

print("\n" + "=" * 70)
print("–≠–¢–ê–ü 3: –ü–†–û–î–í–ò–ù–£–¢–´–ï –ú–ï–¢–û–î–´ (DBSCAN –∏ Hierarchical Clustering)")
print("=" * 70)

# AgglomerativeClustering –≤ sklearn - —ç—Ç–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
# –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ "—Å–Ω–∏–∑—É –≤–≤–µ—Ä—Ö" (agglomerative = hierarchical).

# --- 3.1 DBSCAN ---
print("\n--- 3.1 DBSCAN (Density-Based Spatial Clustering) ---")

# –ü–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ DBSCAN
best_dbscan_score = -1
best_eps = 0
best_min_samples = 0

for eps in [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]:
    for min_samples in [5, 10, 15, 20]:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(X_scaled)
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = list(labels).count(-1)
        if n_clusters >= 2 and n_clusters <= 10:
            mask = labels != -1
            if mask.sum() > n_clusters:
                score = silhouette_score(X_scaled[mask], labels[mask])
                if score > best_dbscan_score:
                    best_dbscan_score = score
                    best_eps = eps
                    best_min_samples = min_samples

print(f"‚úì –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: eps={best_eps}, min_samples={best_min_samples}")

# –û–±—É—á–µ–Ω–∏–µ DBSCAN —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
dbscan_final = DBSCAN(eps=best_eps, min_samples=best_min_samples)
dbscan_labels = dbscan_final.fit_predict(X_scaled)

n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
n_noise_dbscan = list(dbscan_labels).count(-1)

df["cluster_dbscan"] = dbscan_labels

# –ú–µ—Ç—Ä–∏–∫–∏ DBSCAN
mask_dbscan = dbscan_labels != -1
if mask_dbscan.sum() > n_clusters_dbscan and n_clusters_dbscan >= 2:
    dbscan_silhouette = silhouette_score(
        X_scaled[mask_dbscan], dbscan_labels[mask_dbscan]
    )
    dbscan_davies_bouldin = davies_bouldin_score(
        X_scaled[mask_dbscan], dbscan_labels[mask_dbscan]
    )
    dbscan_calinski = calinski_harabasz_score(
        X_scaled[mask_dbscan], dbscan_labels[mask_dbscan]
    )
else:
    dbscan_silhouette = np.nan
    dbscan_davies_bouldin = np.nan
    dbscan_calinski = np.nan

print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã DBSCAN:")
print(f"  ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters_dbscan}")
print(
    f"  ‚Ä¢ –¢–æ—á–∫–∏-—à—É–º (outliers): {n_noise_dbscan} ({n_noise_dbscan / len(df) * 100:.2f}%)"
)
if not np.isnan(dbscan_silhouette):
    print(f"  ‚Ä¢ Silhouette Score: {dbscan_silhouette:.4f}")
print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º (-1 = —à—É–º):")
print(pd.Series(dbscan_labels).value_counts().sort_index())

# --- 3.2 Hierarchical (–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è) –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è ---
print("\n--- 3.2 Hierarchical (–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è) –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è ---")

sample_size = 1000
np.random.seed(RANDOM_STATE)
sample_indices = np.random.choice(len(X_scaled), size=sample_size, replace=False)
X_sample = X_scaled[sample_indices]

linkage_matrix = linkage(X_sample, method="ward")

plt.figure(figsize=(14, 6))
dendrogram(
    linkage_matrix,
    truncate_mode="lastp",
    p=30,
    leaf_rotation=90,
    leaf_font_size=10,
    show_contracted=True,
)
plt.title("–î–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (Ward linkage)")
plt.xlabel("–ò–Ω–¥–µ–∫—Å —Ç–æ—á–∫–∏ / –†–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞")
plt.ylabel("–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ")
plt.axhline(y=50, color="r", linestyle="--", label="–ü–æ—Ä–æ–≥ –æ—Ç—Å–µ—á–µ–Ω–∏—è")
plt.legend()
plt.tight_layout()
plt.savefig("dendrogram.png", dpi=150, bbox_inches="tight")
plt.show()

print("\nüìä [–°–ö–†–ò–ù–®–û–¢ 2: –î–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º–∞ - dendrogram.png]")

# –û–±—É—á–µ–Ω–∏–µ Hierarchical Clustering
print(f"\n–û–±—É—á–µ–Ω–∏–µ Hierarchical Clustering —Å k={optimal_k}...")
hierarchical = AgglomerativeClustering(n_clusters=optimal_k, linkage="ward")
hierarchical_labels = hierarchical.fit_predict(X_scaled)

df["cluster_hierarchical"] = hierarchical_labels

# –ú–µ—Ç—Ä–∏–∫–∏ Hierarchical
hierarchical_silhouette = silhouette_score(X_scaled, hierarchical_labels)
hierarchical_davies_bouldin = davies_bouldin_score(X_scaled, hierarchical_labels)
hierarchical_calinski = calinski_harabasz_score(X_scaled, hierarchical_labels)

print(f"‚úì Hierarchical Clustering –æ–±—É—á–µ–Ω")
print(f"\n–ú–µ—Ç—Ä–∏–∫–∏ Hierarchical Clustering:")
print(f"  ‚Ä¢ Silhouette Score: {hierarchical_silhouette:.4f}")
print(f"  ‚Ä¢ Davies-Bouldin Index: {hierarchical_davies_bouldin:.4f}")
print(f"  ‚Ä¢ Calinski-Harabasz Index: {hierarchical_calinski:.2f}")
print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:")
print(pd.Series(hierarchical_labels).value_counts().sort_index())


# --- 3.3 –°–†–ê–í–ù–ï–ù–ò–ï –° BASELINE –ò –í–´–ë–û–† –ü–û–ë–ï–î–ò–¢–ï–õ–Ø ---
print("\n" + "=" * 60)
print("–°–†–ê–í–ù–ï–ù–ò–ï –ü–†–û–î–í–ò–ù–£–¢–´–• –ú–ï–¢–û–î–û–í –° BASELINE –ò –í–´–ë–û–† –ü–û–ë–ï–î–ò–¢–ï–õ–Ø")
print("=" * 60)

comparison_table = pd.DataFrame(
    {
        "–ú–µ—Ç–æ–¥": ["KMeans (BASELINE)", "DBSCAN", "Hierarchical"],
        "–ö–ª–∞—Å—Ç–µ—Ä–æ–≤": [optimal_k, n_clusters_dbscan, optimal_k],
        "Silhouette": [baseline_silhouette, dbscan_silhouette, hierarchical_silhouette],
        "Davies-Bouldin": [
            baseline_davies_bouldin,
            dbscan_davies_bouldin,
            hierarchical_davies_bouldin,
        ],
        "Calinski-Harabasz": [
            baseline_calinski,
            dbscan_calinski,
            hierarchical_calinski,
        ],
    }
)

print("\n–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–µ—Ç–æ–¥–æ–≤:")
display(comparison_table.round(4))

methods_scores = {
    "KMeans": baseline_silhouette,
    "DBSCAN": dbscan_silhouette if not np.isnan(dbscan_silhouette) else -1,
    "Hierarchical": hierarchical_silhouette,
}

winner = max(methods_scores, key=methods_scores.get)
winner_score = methods_scores[winner]

print(f"\n{'=' * 50}")
print(f"–ü–û–ë–ï–î–ò–¢–ï–õ–¨: {winner}")
print(f"Silhouette Score: {winner_score:.4f}")
print(f"{'=' * 50}")


if winner == "KMeans":
    best_labels = kmeans_labels
elif winner == "DBSCAN":
    best_labels = dbscan_labels
elif winner == "Hierarchical":
    best_labels = hierarchical_labels
else:
    best_labels = kmeans_labels

best_method = winner


# –≠–¢–ê–ü 4: –ú–ï–¢–†–ò–ö–ò –ò –°–†–ê–í–ù–ï–ù–ò–ï

print("\n" + "=" * 70)
print("–≠–¢–ê–ü 4: –ú–ï–¢–†–ò–ö–ò –ò –°–†–ê–í–ù–ï–ù–ò–ï")
print("=" * 70)

# Silhouette Score (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∏–ª—É—ç—Ç–∞)
# Inertia (–∏–Ω–µ—Ä—Ü–∏—è) - —Ç–æ–ª—å–∫–æ –¥–ª—è KMeans
# ARI (Adjusted Rand Index) - –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å "—ç—Ç–∞–ª–æ–Ω–Ω–æ–π" —Ä–∞–∑–º–µ—Ç–∫–æ–π

# --- 4.1 –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ ---
print("\n--- 4.1 –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ ---")

# –°–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—É—é —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è ARI
y_binary = (df["y"] == "yes").astype(int).values

# ARI –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞ (—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π)
ari_kmeans = adjusted_rand_score(y_binary, kmeans_labels)
ari_hierarchical = adjusted_rand_score(y_binary, hierarchical_labels)
ari_dbscan = (
    adjusted_rand_score(y_binary[mask_dbscan], dbscan_labels[mask_dbscan])
    if mask_dbscan.sum() > 0
    else np.nan
)

print("\n" + "=" * 60)
print("–ò–¢–û–ì–û–í–ê–Ø –¢–ê–ë–õ–ò–¶–ê –ú–ï–¢–†–ò–ö")
print("=" * 60)

metrics_table = pd.DataFrame(
    {
        "–ú–µ—Ç–æ–¥": ["KMeans (BASELINE)", "DBSCAN", "Hierarchical"],
        "Silhouette ‚Üë": [
            baseline_silhouette,
            dbscan_silhouette,
            hierarchical_silhouette,
        ],
        "Inertia ‚Üì": [baseline_inertia, "N/A", "N/A"],
        "ARI ‚Üë": [ari_kmeans, ari_dbscan, ari_hierarchical],
        "Davies-Bouldin ‚Üì": [
            baseline_davies_bouldin,
            dbscan_davies_bouldin,
            hierarchical_davies_bouldin,
        ],
        "Calinski-Harabasz ‚Üë": [
            baseline_calinski,
            dbscan_calinski,
            hierarchical_calinski,
        ],
    }
)

display(metrics_table.round(4))

fig, axes = plt.subplots(1, 3, figsize=(15, 5))
methods = ["KMeans", "DBSCAN", "Hierarchical"]
colors = sns.color_palette(PALETTE, len(methods))

# Silhouette
sil_values = [
    baseline_silhouette,
    dbscan_silhouette if not np.isnan(dbscan_silhouette) else 0,
    hierarchical_silhouette,
]
axes[0].bar(methods, sil_values, color=colors)
axes[0].set_title("Silhouette Score (‚Üë –ª—É—á—à–µ)")
axes[0].set_ylabel("Score")
for i, v in enumerate(sil_values):
    axes[0].text(i, v + 0.01, f"{v:.3f}", ha="center", fontsize=10)

# ARI
ari_values = [
    ari_kmeans,
    ari_dbscan if not np.isnan(ari_dbscan) else 0,
    ari_hierarchical,
]
axes[1].bar(methods, ari_values, color=colors)
axes[1].set_title("ARI (‚Üë –ª—É—á—à–µ)")
axes[1].set_ylabel("Score")
for i, v in enumerate(ari_values):
    axes[1].text(i, v + 0.001, f"{v:.4f}", ha="center", fontsize=10)

# Calinski-Harabasz
ch_values = [
    baseline_calinski,
    dbscan_calinski if not np.isnan(dbscan_calinski) else 0,
    hierarchical_calinski,
]
axes[2].bar(methods, ch_values, color=colors)
axes[2].set_title("Calinski-Harabasz Index (‚Üë –ª—É—á—à–µ)")
axes[2].set_ylabel("Score")
for i, v in enumerate(ch_values):
    axes[2].text(i, v + 50, f"{v:.0f}", ha="center", fontsize=10)

plt.tight_layout()
plt.savefig("metrics_comparison.png", dpi=150, bbox_inches="tight")
plt.show()

print("\nüìä [–°–ö–†–ò–ù–®–û–¢ 3: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ - metrics_comparison.png]")

# --- 4.2 –ö—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥ ---
print("\n--- 4.2 –ö–†–ê–¢–ö–ò–ô –í–´–í–û–î –ü–û –ú–ï–¢–†–ò–ö–ê–ú ---")

dbscan_sil_str = (
    f"{dbscan_silhouette:.4f}" if not np.isnan(dbscan_silhouette) else "N/A"
)

print(f"""
–ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç:

1. SILHOUETTE SCORE:
   - KMeans: {baseline_silhouette:.4f}
   - Hierarchical: {hierarchical_silhouette:.4f}
   - DBSCAN: {dbscan_sil_str}

2. INERTIA (—Ç–æ–ª—å–∫–æ KMeans):
   - KMeans: {baseline_inertia:.2f}

3. ARI (—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π y):
   - KMeans: {ari_kmeans:.4f}
   - Hierarchical: {ari_hierarchical:.4f}
""")


# –≠–¢–ê–ü 5: –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –ò –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø

print("\n" + "=" * 70)
print("–≠–¢–ê–ü 5: –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –ò –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø")
print("=" * 70)

# --- 5.1 –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ 2D (PCA) ---
print("\n--- 5.1 –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (PCA) ---")

pca = PCA(n_components=2, random_state=RANDOM_STATE)
X_pca = pca.fit_transform(X_scaled)

print(
    f"‚úì PCA: –æ–±—ä—è—Å–Ω—ë–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è = {pca.explained_variance_ratio_.sum() * 100:.2f}%"
)
print(f"  ‚Ä¢ PC1: {pca.explained_variance_ratio_[0] * 100:.2f}%")
print(f"  ‚Ä¢ PC2: {pca.explained_variance_ratio_[1] * 100:.2f}%")

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# KMeans
scatter1 = axes[0].scatter(
    X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, cmap="Set2", alpha=0.6, s=10
)
axes[0].set_title("KMeans –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è")
axes[0].set_xlabel("PC1")
axes[0].set_ylabel("PC2")
plt.colorbar(scatter1, ax=axes[0], label="–ö–ª–∞—Å—Ç–µ—Ä")
centroids_pca = pca.transform(kmeans_final.cluster_centers_)
axes[0].scatter(
    centroids_pca[:, 0],
    centroids_pca[:, 1],
    c="red",
    marker="X",
    s=200,
    edgecolors="black",
    linewidths=2,
    label="–¶–µ–Ω—Ç—Ä–æ–∏–¥—ã",
)
axes[0].legend()

# DBSCAN
scatter2 = axes[1].scatter(
    X_pca[:, 0], X_pca[:, 1], c=dbscan_labels, cmap="Set2", alpha=0.6, s=10
)
axes[1].set_title("DBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è")
axes[1].set_xlabel("PC1")
axes[1].set_ylabel("PC2")
plt.colorbar(scatter2, ax=axes[1], label="–ö–ª–∞—Å—Ç–µ—Ä (-1=—à—É–º)")

# Hierarchical
scatter3 = axes[2].scatter(
    X_pca[:, 0], X_pca[:, 1], c=hierarchical_labels, cmap="Set2", alpha=0.6, s=10
)
axes[2].set_title("Hierarchical –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è")
axes[2].set_xlabel("PC1")
axes[2].set_ylabel("PC2")
plt.colorbar(scatter3, ax=axes[2], label="–ö–ª–∞—Å—Ç–µ—Ä")

plt.tight_layout()
plt.savefig("clusters_pca.png", dpi=150, bbox_inches="tight")
plt.show()

print("\nüìä [–°–ö–†–ò–ù–®–û–¢ 4: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ PCA - clusters_pca.png]")

# --- 5.2 –ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ---
print("\n--- 5.2 –ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ---")

df_analysis = df_cluster.copy()
df_analysis["Cluster"] = best_labels

numeric_features = ["age", "campaign", "pdays", "previous"]

cluster_profiles_numeric = df_analysis.groupby("Cluster")[numeric_features].agg(
    ["mean", "median", "std"]
)
display(cluster_profiles_numeric.round(2))

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for i, feature in enumerate(numeric_features):
    russian_name = COLUMN_TRANSLATOR.get(feature, feature)
    cluster_means = df_analysis.groupby("Cluster")[feature].mean()
    bars = axes[i].bar(
        cluster_means.index,
        cluster_means.values,
        color=sns.color_palette(PALETTE, optimal_k),
    )
    axes[i].set_title(f"–°—Ä–µ–¥–Ω–∏–π {russian_name} –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º")
    axes[i].set_xlabel("–ö–ª–∞—Å—Ç–µ—Ä")
    axes[i].set_ylabel(russian_name)
    for bar, val in zip(bars, cluster_means.values):
        axes[i].text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 0.5,
            f"{val:.1f}",
            ha="center",
            fontsize=10,
        )

plt.tight_layout()
plt.savefig("cluster_profiles_numeric.png", dpi=150, bbox_inches="tight")
plt.show()

print("\nüìä [–°–ö–†–ò–ù–®–û–¢ 5: –ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (—á–∏—Å–ª–æ–≤—ã–µ) - cluster_profiles_numeric.png]")

categorical_features = ["job", "marital", "education", "poutcome"]

fig, axes = plt.subplots(2, 2, figsize=(16, 12))
axes = axes.flatten()

for i, feature in enumerate(categorical_features):
    russian_name = COLUMN_TRANSLATOR.get(feature, feature)
    cross_tab = pd.crosstab(
        df_analysis["Cluster"], df_analysis[feature], normalize="index"
    )
    cross_tab.plot(kind="bar", ax=axes[i], colormap=PALETTE, width=0.8)
    axes[i].set_title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ "{russian_name}" –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º')
    axes[i].set_xlabel("–ö–ª–∞—Å—Ç–µ—Ä")
    axes[i].set_ylabel("–î–æ–ª—è")
    axes[i].legend(title=russian_name, bbox_to_anchor=(1.02, 1), loc="upper left")
    axes[i].tick_params(axis="x", rotation=0)

plt.tight_layout()
plt.savefig("cluster_profiles_categorical.png", dpi=150, bbox_inches="tight")
plt.show()

print(
    "\nüìä [–°–ö–†–ò–ù–®–û–¢ 6: –ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ) - cluster_profiles_categorical.png]"
)

# --- 5.3 –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ---
print("\n--- 5.3 –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ---")

unique_clusters = sorted([c for c in df_analysis["Cluster"].unique() if c != -1])

for cluster_id in unique_clusters:
    cluster_data = df_analysis[df_analysis["Cluster"] == cluster_id]
    print(f"\n{'=' * 50}")
    print(f"–ö–õ–ê–°–¢–ï–† {cluster_id}")
    print(f"{'=' * 50}")
    print(
        f"–†–∞–∑–º–µ—Ä: {len(cluster_data)} –∫–ª–∏–µ–Ω—Ç–æ–≤ ({len(cluster_data) / len(df_analysis) * 100:.1f}%)"
    )
    print(f"\n–ß–∏—Å–ª–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:")
    print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π –≤–æ–∑—Ä–∞—Å—Ç: {cluster_data['age'].mean():.1f} –ª–µ—Ç")
    print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª-–≤–æ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤: {cluster_data['campaign'].mean():.1f}")
    top_job = (
        cluster_data["job"].mode().iloc[0]
        if len(cluster_data["job"].mode()) > 0
        else "N/A"
    )
    top_marital = (
        cluster_data["marital"].mode().iloc[0]
        if len(cluster_data["marital"].mode()) > 0
        else "N/A"
    )
    print(f"\n–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:")
    print(f"  ‚Ä¢ –ü—Ä–µ–æ–±–ª–∞–¥–∞—é—â–∞—è –ø—Ä–æ—Ñ–µ—Å—Å–∏—è: {top_job}")
    print(f"  ‚Ä¢ –ü—Ä–µ–æ–±–ª–∞–¥–∞—é—â–µ–µ —Å–µ–º. –ø–æ–ª–æ–∂–µ–Ω–∏–µ: {top_marital}")

print("\n" + "=" * 60)
print("–°–í–Ø–ó–¨ –ö–õ–ê–°–¢–ï–†–û–í –° –¶–ï–õ–ï–í–û–ô –ü–ï–†–ï–ú–ï–ù–ù–û–ô (–°–û–ì–õ–ê–°–ò–ï –ù–ê –í–ö–õ–ê–î)")
print("=" * 60)

df_analysis["y"] = df["y"]
conversion_by_cluster = df_analysis.groupby("Cluster")["y"].apply(
    lambda x: (x == "yes").sum() / len(x) * 100
)

print("\n–ö–æ–Ω–≤–µ—Ä—Å–∏—è (% —Å–æ–≥–ª–∞—Å–∏–π) –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:")
for cluster_id, conv in conversion_by_cluster.items():
    print(f"  –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {conv:.2f}%")

plt.figure(figsize=(10, 6))
bars = plt.bar(
    conversion_by_cluster.index,
    conversion_by_cluster.values,
    color=sns.color_palette(PALETTE, optimal_k),
)
plt.title("–ö–æ–Ω–≤–µ—Ä—Å–∏—è (—Å–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –≤–∫–ª–∞–¥) –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º", fontsize=14)
plt.xlabel("–ö–ª–∞—Å—Ç–µ—Ä")
plt.ylabel("% —Å–æ–≥–ª–∞—Å–∏–π")
for bar, val in zip(bars, conversion_by_cluster.values):
    plt.text(
        bar.get_x() + bar.get_width() / 2,
        bar.get_height() + 0.5,
        f"{val:.1f}%",
        ha="center",
        fontsize=12,
        fontweight="bold",
    )

plt.tight_layout()
plt.savefig("cluster_conversion.png", dpi=150, bbox_inches="tight")
plt.show()


# –≠–¢–ê–ü 6: –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö –ò–õ–ò –†–ï–ó–£–õ–¨–¢–ê–¢–û–í

print("\n" + "=" * 70)
print("–≠–¢–ê–ü 6: –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ò –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ô")
print("=" * 70)

# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–∏–ª—É—ç—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏
sample_silhouettes = silhouette_samples(X_scaled, best_labels)
df_analysis["silhouette"] = sample_silhouettes

print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏–ª—É—ç—Ç–∞ –ø–æ —Ç–æ—á–∫–∞–º:")
print(f"  ‚Ä¢ –ú–∏–Ω: {sample_silhouettes.min():.4f}")
print(f"  ‚Ä¢ –ú–∞–∫—Å: {sample_silhouettes.max():.4f}")
print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: {sample_silhouettes.mean():.4f}")
print(f"  ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞: {np.median(sample_silhouettes):.4f}")

bad_points = df_analysis[df_analysis["silhouette"] < 0]
print(
    f"\n‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {len(bad_points)} —Ç–æ—á–µ–∫ —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º —Å–∏–ª—É—ç—Ç–æ–º ({len(bad_points) / len(df_analysis) * 100:.2f}%)"
)
print("–≠—Ç–æ —Ç–æ—á–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –±–ª–∏–∂–µ –∫ —Å–æ—Å–µ–¥–Ω–µ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É, —á–µ–º –∫ —Å–≤–æ–µ–º—É.")


plt.figure(figsize=(12, 6))
for cluster_id in unique_clusters:
    cluster_silhouettes = sample_silhouettes[best_labels == cluster_id]
    y_lower = cluster_id * (len(df_analysis) // optimal_k + 10)
    y_upper = y_lower + len(cluster_silhouettes)
    color = sns.color_palette(PALETTE, optimal_k)[cluster_id]
    plt.fill_betweenx(
        np.arange(y_lower, y_upper),
        0,
        np.sort(cluster_silhouettes),
        facecolor=color,
        edgecolor=color,
        alpha=0.7,
    )
    plt.text(
        -0.05,
        y_lower + len(cluster_silhouettes) / 2,
        f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}",
        fontsize=10,
    )

plt.axvline(
    x=sample_silhouettes.mean(),
    color="red",
    linestyle="--",
    label=f"–°—Ä–µ–¥–Ω–∏–π —Å–∏–ª—É—ç—Ç ({sample_silhouettes.mean():.3f})",
)
plt.xlabel("–°–∏–ª—É—ç—Ç")
plt.ylabel("–¢–æ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∏–ª—É—ç—Ç–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º")
plt.legend()
plt.tight_layout()
plt.savefig("silhouette_distribution.png", dpi=150, bbox_inches="tight")
plt.show()

print("\nüìä [–°–ö–†–ò–ù–®–û–¢ 8: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∏–ª—É—ç—Ç–∞ - silhouette_distribution.png]")


print("\n--- –ü—Ä–∏–º–µ—Ä—ã —Ç–æ—á–µ–∫ —Å –Ω–∏–∑–∫–∏–º —Å–∏–ª—É—ç—Ç–æ–º ---")
print("\n–¢–æ–ø-5 —Ç–æ—á–µ–∫ —Å —Å–∞–º—ã–º –Ω–∏–∑–∫–∏–º —Å–∏–ª—É—ç—Ç–æ–º:")
worst_points = df_analysis.nsmallest(5, "silhouette")[
    clustering_features + ["Cluster", "silhouette"]
]
display(worst_points)

# –≠–¢–ê–ü 7: –†–ï–ü–†–û–î–£–¶–ò–†–£–ï–ú–û–°–¢–¨

print("\n" + "=" * 70)
print("–≠–¢–ê–ü 7: –†–ï–ü–†–û–î–£–¶–ò–†–£–ï–ú–û–°–¢–¨")
print("=" * 70)

print(f"\n--- –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π random_state ---")
print(f"–î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω: random_state = {RANDOM_STATE}")

print("\n--- –í–µ—Ä—Å–∏–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫ ---")
print(f"  ‚Ä¢ Python:       {platform.python_version()}")
print(f"  ‚Ä¢ pandas:       {pd.__version__}")
print(f"  ‚Ä¢ numpy:        {np.__version__}")
print(f"  ‚Ä¢ seaborn:      {sns.__version__}")
print(f"  ‚Ä¢ scikit-learn: {sklearn.__version__}")
print(f"  ‚Ä¢ matplotlib:   {plt.matplotlib.__version__}")

print("\n--- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ ---")
print(f"  ‚Ä¢ –û–°: {platform.system()} {platform.release()}")
print(f"  ‚Ä¢ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {platform.machine()}")

df_results = df.copy()
df_results["cluster"] = best_labels
df_results.to_csv("clustered_clients.csv", index=False)
print("\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'clustered_clients.csv'")
